ðŸŽ§ Sparkify Data Pipeline with Apache Airflow
=============================================

Author: Saad Iqbal
Date: 23/12/2025

Project Overview
----------------

This project builds a **production-grade data pipeline** for Sparkify, a fictional music streaming company.The pipeline uses **Apache Airflow** to automate the process of:

*   Loading JSON data from **Amazon S3**
    
*   Staging data into **Amazon Redshift**
    
*   Transforming data into **fact and dimension tables**
    
*   Running **data quality checks** to ensure reliability
    

The DAG is modular, extensible, and uses **custom Airflow operators**.

Data Sources
------------

The pipeline processes two datasets stored in S3:

*   **Song data**: metadata about songs and artists
    
*   **Log data**: user activity logs generated by the Sparkify app
    

Pipeline Architecture
---------------------

### DAG Name

`   udacity_sparkify_pipeline   `

### Task Flow

1.  **Begin\_execution** â€“ Dummy start task
    
2.  **Stage\_events** â€“ Load event log data from S3 to Redshift
    
3.  **Stage\_songs** â€“ Load song data from S3 to Redshift
    
4.  **Load\_songplays\_fact\_table** â€“ Populate fact table
    
5.  **Load dimension tables** (in parallel):
    
    *   users
        
    *   songs
        
    *   artists
        
    *   time
        
6.  **Run\_data\_quality\_checks** â€“ Validate data integrity
    
7.  **Stop\_execution** â€“ Dummy end task
    

Custom Operators
----------------

The pipeline uses the following custom operators:

*   **StageToRedshiftOperator**
    
*   **LoadFactOperator**
    
*   **LoadDimensionOperator**
    
*   **DataQualityOperator**
    

These operators are located in:

`   plugins/operators/   `

SQL helper queries are defined in:

`   plugins/helpers/sql_queries.py   `

Data Quality Checks
-------------------

The pipeline validates that:

*   Fact and dimension tables are **not empty**
    
*   Primary key columns contain **no NULL values**
    

The pipeline **fails automatically** if any check does not meet expectations.

How to Run the Pipeline
-----------------------

1.  Start the Airflow web server and scheduler
    
2.  Open the Airflow UI
    
3.  Enable the DAG: udacity\_sparkify\_pipeline
    
4.  Trigger the DAG manually or allow scheduled execution
    
5.  Monitor execution via **Graph View** or **Grid View**
    

Project Structure
-----------------

`   .  â”œâ”€â”€ dags/  â”‚   â””â”€â”€ final_project.py  â”œâ”€â”€ plugins/  â”‚   â”œâ”€â”€ operators/  â”‚   â”‚   â”œâ”€â”€ stage_redshift.py  â”‚   â”‚   â”œâ”€â”€ load_fact.py  â”‚   â”‚   â”œâ”€â”€ load_dimension.py  â”‚   â”‚   â””â”€â”€ data_quality.py  â”‚   â””â”€â”€ helpers/  â”‚       â””â”€â”€ sql_queries.py  â”œâ”€â”€ screenshots/  â”‚   â”œâ”€â”€ sparkify_dag_list_view.png  â”‚   â”œâ”€â”€ sparkify_dag_graph.png  â”‚   â””â”€â”€ sparkify_dag_grid_view.png  â””â”€â”€ README.md   `

Screenshots
-----------

The following screenshots are included to demonstrate successful execution:

*   DAG list view
    
*   DAG graph view (task dependencies)
    
*   DAG grid view (successful runs)
    

Technologies Used
-----------------

*   Apache Airflow
    
*   Amazon S3
    
*   Amazon Redshift
    
*   PostgreSQL
    
*   Python
    

Conclusion
----------

This project demonstrates how to design and orchestrate a robust ETL pipeline using Airflow, with proper staging, transformation, and validation steps.The modular design allows for easy scaling and extension as Sparkifyâ€™s data needs grow.
